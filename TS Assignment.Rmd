---
title: "Time Series Assignment"
output: html_document
date: "2025-07-09"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(forecast)
library(bootUR)
library(dplyr)
```

# Data Loading and Preparation

```{r}

data = read.csv("2025-06-Elec-train.csv")

# Changing column names for convenience
colnames(data) = c("time", "consumption", "temperature")
nrow(data)

```


# Data Overview


```{r}
summary(data)
```

#### Checking NAs
```{r}
data[is.na(data$consumption), ]
```
**Observations: **
- We can see that all the NA values for consumption are the values that we are trying to predict with our model.
- In addition, we can identify possible outliers for the consumption variable, since we can see we have some values that are zero.

#### Checking Outliers

Z-score
```{r}
# Calculate Z-scores
z_scores = (data$consumption - mean(data$consumption, na.rm = TRUE)) / sd(data$consumption, na.rm = TRUE)

# Considering threshold = 2
outlier_index = which(abs(z_scores) > 2)
outliers = data[outlier_index,"consumption"]
outliers
```



Observation with zero consumption
```{r}
data[!is.na(data$consumption) & data$consumption == 0, ]
```


**Observations: **
- Observations with zero consumption:  We can see we have very few observations that have consumption zero that have a time frame of approximately 2 hours. It could indicate a blackout during that period or problem with the data collection. For these points, we will attribute the lower average bigger than zero.

```{r}
min_value = summary(data[!is.na(data$consumption) & data$consumption != 0, "consumption"])[1]
data[!is.na(data$consumption) & data$consumption == 0, "consumption"] = min_value
boxplot(data$consumption)
```

# Time Series Data

```{r}

ts = ts(na.omit(data), frequency = 96)

# Preparing future temperature data for prediction with covariate
ts_future_temp = data[is.na(data$consumption),"temperature"]

head(ts)
head(ts_future_temp)
length(ts_future_temp)
summary(ts)
```

# Time Series EDA

```{r}
plot(decompose(ts[,"consumption"]))
ggseasonplot(ts[,"consumption"])

ggAcf(ts[,"consumption"])
ggPacf(ts[,"consumption"])
```

**Observations**
- Clear seasonal pattern of 1 day (24h*60min / 15min = 96)
- Different energy consumption levels during the day (lowest in the night, increase in the morning and peak end of the day)
- There seem to be no apparent trend, except for a slight decrease at the end that does not seem to be very significant.
- Variance seems to be stable, so we assume homoscedasticity.
- The time series is clearly not stationary.

(!) We see some trends that are slight shifted on the last abrupt descend.



# Time Series Feature Engineering

Using POSIXct library to better interpret the days of the week and time
```{r}
data[,"time"] = as.POSIXct(data[,"time"], format="%m/%d/%Y %H:%M")
```




# Weekend Analysis

```{r}

data["weekday"] = weekdays(data[,"time"])
data["is_weekend"] = ifelse(data[,"weekday"] %in% c("Thursday", "Friday"), 1, 0)

data_cpy = data
data_cpy[,"weekday"] = factor(data[,"weekday"], levels = c("Monday","Tuesday","Wednesday","Thursday","Friday","Saturday","Sunday"), ordered = TRUE)


data[,"weekday0"] = 0
data[,"weekday1"] = 0
data[,"weekday2"] = 0

data[,"weekday0"] = ifelse(data[,"weekday"] %in% c("Monday", "Wednesday", "Friday"),0,1)
data[,"weekday1"] = ifelse(data[,"weekday"] %in% c("Wednesday", "Thursday", "Sunday"),1,0)
data[,"weekday2"] = ifelse(data[,"weekday"] %in% c("Friday", "Saturday", "Sunday"),1,0)


boxplot(consumption ~ weekday, data = data_cpy, main = "Consumption per Weekday", las = 3, xlab = "")
data
```



Flagging periods of low/medium and high demand
```{r}

# 0 - 23:15 - 08:00  (Low)
# 1 - 08:15 - 17:00  (Peak 1)
# 2 - 17:15 - 23:00  (Peak 2)

data[,"demand"] = ifelse((format(data[,"time"], "%H:%M") >= "08:15") & (format(data[,"time"], "%H:%M") <= "17:00"),1,
                         ifelse((format(data[,"time"], "%H:%M") >= "17:15") & (format(data[,"time"], "%H:%M") <= "23:00"),2,0)) 

boxplot(consumption ~ demand, data = data,
        main = "Demand Category Distribution",
        xlab = "Demand Category",
        ylab = "Consumption")

```
**Observations: ** 
- We can see that for the category 2 there are quite a lot of outliers, which can explain some days where the abrupt drop from demand high peak 2 to low demand happened BEFORE
the pre-defined cutoff time (23:15)

Option 1: Re-label those categories to the correct one. Problem = Impossible to use it as external feature, since we will not know the consumption to correctly label the demand.
Option 2: Treat them as outliers. This approach seems to be more adequate, since we can keep classifying the demand according to the time of the day.


Option 2: Treat them as outliers.
```{r}

# 1) Identify those observations that are mislabeled
# 2) Replace those days with the mean for all the days

# For those observations that have that shift, they will be changed with the mean
time_means = data %>% group_by(format(time, "%H:%M:%S")) %>%  summarise(mean_consump = mean(consumption, na.rm = TRUE))
colnames(time_means) = c("time", "mean")

thrid_quantiles =data %>% group_by(format(time, "%H:%M:%S")) %>%  summarise(third_quantile = quantile(consumption, 0.75, na.rm = TRUE))
colnames(thrid_quantiles) = c("time", "third quantile")

# Add a table with the observation mean for an observation considering the time only and not the days
data[,"time_HMS"] = format(data[,"time"],"%H:%M:%S")
data = left_join(data, time_means, by = c("time_HMS" = "time"))
data = left_join(data, thrid_quantiles, by = c("time_HMS" = "time"))
#data[,c("time", "time_HMS", "consumption", "mean")]


#23:00 mean
high_peak_mean = pull(time_means[time_means$time == "23:00:00" , "mean"])
medium_peak_mean = pull(time_means[time_means$time == "08:15:00" , "mean"])
low_peak_mean = pull(time_means[time_means$time == "01:15:00" , "mean"])


# Replacing those observations with the mean calculated before (with 5% tolerance)
data[,"consumption"] = ifelse((data$consumption < high_peak_mean) & (data$demand == 2),
                              data[,"mean"],
                              data[,"consumption"])

data[,"consumption"] = ifelse((data$consumption < medium_peak_mean) & (data$demand == 1),
                              data[,"mean"],
                              data[,"consumption"])

data[,"consumption"] = ifelse((data$consumption < low_peak_mean) & (data$demand == 0),
                              data[,"mean"],
                              data[,"consumption"])


# Higher values outleirs
# Removing 20% of the variability of the highest values
data[,"consumption"] = ifelse((data$consumption > data[,"third quantile"]) & (data$demand == 2),
                              data[,"third quantile"],
                              data[,"consumption"])

data[,"consumption"] = ifelse((data$consumption > data[,"third quantile"]) & (data$demand == 1),
                              data[,"third quantile"],
                              data[,"consumption"])

data[,"consumption"] = ifelse((data$consumption > data[,"third quantile"]) & (data$demand == 0),
                              data[,"third quantile"],
                              data[,"consumption"])





summary(data[data$demand == 2,"consumption"])["3rd Qu."]
summary(data[data$demand == 1,"consumption"])
summary(data[data$demand == 0,"consumption"])




boxplot(consumption ~ demand, data = data,
        main = "Demand Category Distribution after Transofmration",
        xlab = "Demand Category",
        ylab = "Consumption")



```



Encoding the demand
```{r}
# Encoding demand column
data[,"demand0"] = ifelse(data[,"demand"] == 0, 0, 1)
data[,"demand1"] = ifelse(data[,"demand"] == 2, 1, 0)
```



# Preparing XREG covariates that might be used
```{r}

xregs = data.frame(matrix(ncol = 0, nrow = 96))

# Temperature
xregs[,"tempearture"] = ts_future_temp

# Day of the week (ordinal)
xregs[,"weekday"] = factor(data[is.na(data$consumption),"weekday"], levels = c("Monday","Tuesday","Wednesday","Thursday","Friday","Saturday","Sunday"), ordered = TRUE)

# Day of the week (one-hot encoded)
xregs[,"weekday0"] = ifelse(weekdays(as.Date("2010-02-21")) %in% c("Monday", "Wednesday", "Friday"),0,1)
xregs[,"weekday1"] = ifelse(weekdays(as.Date("2010-02-21")) %in% c("Wednesday", "Thursday", "Sunday"),1,0)
xregs[,"weekday2"] = ifelse(weekdays(as.Date("2010-02-21")) %in% c("Friday", "Saturday", "Sunday"),1,0)



# Demand level (ordinal)
xregs[,"demand"] = ifelse((format(data[is.na(data$consumption), "time"], "%H:%M") >= "08:15") & (format(data[is.na(data$consumption), "time"], "%H:%M") <= "17:00"),1,
                         ifelse((format(data[is.na(data$consumption), "time"], "%H:%M") >= "17:15") & (format(data[is.na(data$consumption), "time"], "%H:%M") <= "23:00"),2,0)) 

# Demand level (one-hot encoded)
xregs[,"demand0"] = ifelse(xregs[,"demand"] == 0, 0, 1)
xregs[,"demand1"] = ifelse(xregs[,"demand"] == 2, 1, 0)

```


Preparing Final data for modeling
```{r}
data_clean = na.omit(data)



ts = ts(data_clean[,-1], freq = 96) # Removing time columns

summary(ts)
ggseasonplot(ts[,"consumption"], main = "Seasonal Full Dataset")
plot(ts[,c("consumption", "temperature")], main = "Full Data Set")
plot(decompose(ts[,"consumption"]))
```


Split into train/test data set to select the best model (and then cross validation and/or hyper parameter for the best one)

```{r}
# Roughly 80/20 split
ts_train = window(ts, end = c(40, 96))
ts_test = window(ts, start = c(41,1))
test_horizon = length(ts_test[,"consumption"])



summary(ts_train)
ggseasonplot(ts_train[,"consumption"], main = "Train Data Set Seasonal Plot")
plot(ts_train[,c("consumption", "temperature")], main = "Train Data Set")
plot(decompose(ts_train[,"consumption"]))



```



# Modeling

## Without Covariate Temperature
### Support Functions


```{r}
# This function plots the predicted forecast and the true forecastg from the test set
plot_forecast = function(train, test, pred, xlim = c(40, 55), title)
{ 
  plot(train, xlim = xlim, main = title)
  lines(pred, col = "blue")
  lines(test, col = "red")
  legend("topright", 
       legend = c("Train", "Forecast", "Test"), 
       col = c("black", "blue", "red"), 
       lty = 1)
}
```


```{r}
# Manually check residuals for some models (like neural networks)
check_residuals = function(ypred, ytrue, freq){
  
  
  residuals = ypred - ytrue
  
  ts_res= ts(residuals,frequency = freq)
  
  
  plot(ts_res)
  
  print(Box.test(ts_res, type = "Ljung-Box"))
  print(ggAcf(ts_res))
  print(ggPacf(ts_res))
  
}
```

```{r}
# Forecast function for machine learning models
predMl = function(h, freq, pred_start,train, model, matrix = FALSE)
{
  pred = rep(NULL, h)

  # The first prediction will be the last row of  the training data set. 
  newdata = t(tail(train, freq))
  for (t in 1:(h)){
    
    # To deal with the cases where we need to have a matrix for prediction (like in SVM)
    if(matrix == TRUE)
    {
      newdata = matrix(newdata,1,freq)
    }
    pred[t] = predict(model, newdata = newdata)
    
    # The following element forecast will be from the second element until the previously predicted value (and so on)
    newdata = c(newdata[-1],pred[t])
  
  }
  
  # Finally, we transform the predictions into a Time-Series again
  pred_ts = ts(pred, start = pred_start, freq = freq)
  
  return(pred_ts)
  
}


```



Data frame that will contain all the errors for comparison
```{r}
errors_df = data.frame(
  method = character(),
  covariate = numeric(),
  MSE = numeric()
)
```

```{r}
# Support function to calcualte the error with train and test set

mse_error = function(y_pred, y_test){
  mse = sqrt(mean((y_pred-y_test)^2))
  return (mse)
}
```


### Exponential Smoothing


```{r}
#ses_cv = function(x, h){forecast(ses(x, h = 96), h=h)}
#err1_ses = tsCV(ts_train[,"consumption"], ses_cv, h = 96)
#errors_df = rbind(errors_df, new_error_row(error = err1_ses, 
#                                           model = "SES", 
#                                          is_covar = FALSE))

# Train and Forecast
fit_ses = ses(ts_train[,"consumption"], h = test_horizon)
pred = fit_ses$mean

# Save Error
mse = mse_error(fit_ses$mean, ts_test[,"consumption"])
cat("SES MSE: ",  mse, "\n")
errors_df = rbind(errors_df, data.frame(
                                        method = "SES",
                                        covariate = FALSE,
                                        MSE = mse))

# Plot Forecast
plot_forecast(train = ts_train[,"consumption"], test = ts_test[,"consumption"], pred = pred, xlim = c(40, 55), title = "SES Forecast")
```

### SARIMA

## Differenciating
```{r}
diff_ts = diff(ts_train[,"consumption"], lag = 96)

plot(ts_train[,"consumption"], main = "Original TS")
plot(diff_ts, main = "Seasonal diff with leg 96")
ggAcf(diff_ts, lag = 96*4)
ggPacf(diff_ts, lag = 96*4)


# Checking with ADF test
adf(diff_ts, criterion = "AIC")
```


**Observations**
- The series seems to be stationary despite some spikes present after applying a seasonal differentiation.
- The ADF test provides us with more statistically significant results to reject the non-stationary hypothesis (p-value << 0.01), agreeing with the visual analysis. 
- We can see a quick exponential decrease in the ACF and possibly at the PACF as well, although with a slower decay.
- MA models: q = [0,15]     = by looking at the most significant spikes at the first few lags
             Q = 1          = due to the big spike at the first period 96
             
- AR models: p = [0, 8]    = bigger spiked in the first few lags
             P = [1, 4]     = given the recurrent decreasing spiked at each lag
 

#### Auto.Sarima
```{r}
auto_fit = auto.arima(ts_train[,"consumption"])
auto_fit
checkresiduals(auto_fit)


pred = forecast(auto_fit, h = test_horizon)$mean

# Plot Forecast
plot_forecast(train = ts_train[,"consumption"], test = ts_test[,"consumption"], pred = pred, xlim = c(40, 55), title = "Auto.arima Forecast")
```

**Observations**
- Auto arima results indicate a move towards both an AR and MA models with a seasonal differentiation only. However, by analysing the ACF, we can see a big spike in the lag 96, indicating the need to consider that in the seasonal order of the SARIMA. For that reason, since we have the big spike in the ACF, we will add a Q = 1.

```{r}
# Fit
arima_fit = Arima(ts_train[,"consumption"], order = c(1,0,2), seasonal = c(0,1,1))

# Forecast
pred = forecast(arima_fit, h = test_horizon)$mean
      
# Calculate and store error
mse = mse_error(pred, ts_test[,"consumption"])
cat("Arima (1,0,2)(0,1,1) MSE", mse, "\n")

errors_df = rbind(errors_df, data.frame(
                                        method = "Arima (1,0,2)(0,1,1)[96]",
                                        covariate = FALSE,
                                        MSE = mse))

# Check residuals
checkresiduals(arima_fit)

# Plotting Forecast
plot_forecast(train = ts_train[,"consumption"], test = ts_test[,"consumption"], pred = pred, title = "Arima (1,0,2)(0,1,1)[96]")
```


**Observations**
- We can see that after increasing the seasonal Q order to 1, we managed to get rid of the big spike at lag 96 and also account for most of the correlations in the residuals as we can see
the statically significant p-value (> 0.5) indicating the presence of White Noise.

- The remaining significant spikes could be due to the presence of undetected/unfiltered outliers.

- To try to improve our model, we test empirically a few combinations of SARIMA comparing the AIC for models that have p-value > 0.5.

- AIC=19114.35   AICc=19114.37   BIC=19145.49

#### Hypertuning SARIMA

```{r}

# Data frame containing the results of the tuning parameters for SARIMA
arima_tuning_df = data.frame(
  p = character(),
  q = numeric(),
  AIC = numeric(),
  AICc = numeric(),
  BIC = numeric(),
  error = numeric()
)

for(p in (1:5)){
  for(q in (2:5)){
    # Train current model
    tuning_arima_fit = Arima(ts_train[,"consumption"], order = c(p,0,q), seasonal = c(0,1,1))
    
    # Check residuals
    residuals = checkresiduals(tuning_arima_fit)
    
    # Save results if p-value > 0.05
    if(residuals$p.value > 0.05)
    {
      # Forecast
      pred = forecast(tuning_arima_fit, h = test_horizon)$mean
      
      # Calculate the error
      mse = mse_error(pred, ts_test[,"consumption"])
      
      # Save results in the data frame
      arima_tuning_df = rbind(arima_tuning_df, data.frame(
                                                          p = p,
                                                          q = q,
                                                          AIC = tuning_arima_fit$aic,
                                                          AICc = tuning_arima_fit$aicc,
                                                          BIC = tuning_arima_fit$bic,
                                                          error = mse))
          
      cat("AIC: ", tuning_arima_fit$aic, "  ")
      cat("AICc: ", tuning_arima_fit$aicc, "  ")
      cat("BIC: ", tuning_arima_fit$bic, "\n")
      cat("MSE: ", mse, "\n")
    }

  }
}


arima_tuning_df

write.xlsx(arima_tuning_df, "sarima_hypertuning.xlsx", colNames = TRUE)

```


**Observations**
- After testing with different configurations, the model that apparently represents the training data is SARIMA(1,0,2)(0,1,1)[96] as it seems to capture most of the characteristics of the time series and not statistically significant correlation in the residuals as we can see by looking at the p-value > 0.05, indicating that we can reject the null hypothesis of correlation in the residuals.



## TSLM


```{r}
# Fit
tslm_fit = tslm(consumption~trend+season,data= ts_train)

# Forecast
pred = forecast(tslm_fit, h = test_horizon)$mean

#Calculate error
mse =  mse_error(pred, ts_test[,"consumption"])
cat("tsml MSE: ", mse, "\n")
errors_df = rbind(errors_df, data.frame(
                                        method = "TSLM",
                                        covariate = FALSE,
                                        MSE = mse))

checkresiduals(tslm_fit)

plot_forecast(train = ts_train[,"consumption"], test = ts_test[,"consumption"], pred = pred, title = "TSML")

```


## Machine Learning Models

### ML Data Preparation

#### New ML Compatible dataset
```{r}
# We transform everything in vector
ts_vector = as.vector(ts_train[,"consumption"])

# Get the first row (in vector form) to start the iteration
ts_ml = ts_vector[1:97]

# For each element of t he vector, generate a new row with a response columns added
for(i in 1:(length(ts_vector)-97))
{
  ts_ml = rbind(ts_ml, as.vector(ts_vector[(i+1):(i+97)]))
}

```


### Random Forest

```{r}
library(randomForest)

# Fit
rf_fit = randomForest(x = ts_ml[,-97], y=ts_ml[,97])

# Forecast
pred = predMl(h = test_horizon, freq = 96, pred_start = c(41,1), train = ts_train[,"consumption"], model = rf_fit)

# Calculate and store error
mse = mse_error(pred, ts_test[,"consumption"])
cat("Random Forest MSE: ", mse)
errors_df = rbind(errors_df, data.frame(
                                        method = "Random Forest",
                                        covariate = FALSE,
                                        MSE = mse))

# Plot forecast
plot_forecast(train = ts_train[,"consumption"], test = ts_test[,"consumption"], pred = pred, title = "Random Forest")

# Check Residuals
check_residuals(ypred = pred, ytrue = ts_test[,"consumption"], freq = 96)
```

### Gradient Boosting

```{r}
library(xgboost)

# Fit
xg_fit = xgboost(data = ts_ml[,-97], label = ts_ml[,97],
                                        max_depth = 10, 
                                        eta = .5, 
                                        nrounds = 50,
                                        nthread = 2, 
                                        objective = "reg:squarederror", 
                                        verbose = FALSE)

# Forecast
pred = predMl(h = test_horizon, freq = 96, pred_start = c(41,1), train = ts_train[,"consumption"], model = xg_fit, matrix = TRUE)

# Calculate and store error
mse = mse_error(pred, ts_test[,"consumption"])
cat("XGBoost MSE: ", mse)
errors_df = rbind(errors_df, data.frame(
                                        method = "XGBoost",
                                        covariate = FALSE,
                                        MSE = mse))

# Plotting Forecast
plot_forecast(train = ts_train[,"consumption"], test = ts_test[,"consumption"], pred = pred, title = "XGBoost")


# Check residuals
check_residuals(ypred = pred, ytrue = ts_test[,"consumption"], freq = 96)
```


### SVM

```{r}
library(e1071)

# Fit
svm_fit = svm(x = ts_ml[,-97], y = ts_ml[,97])

# Forecast
pred = predMl(h = test_horizon, freq = 96, pred_start = c(41,1), train = ts_train[,"consumption"], model = svm_fit, matrix = TRUE)

# Calculate and store error
mse = mse_error(pred, ts_test[,"consumption"])
cat("SVM MSE: ", mse)
errors_df = rbind(errors_df, data.frame(
                                        method = "SVM",
                                        covariate = FALSE,
                                        MSE = mse))

# Plotting Forecast
plot_forecast(train = ts_train[,"consumption"], test = ts_test[,"consumption"], pred = pred, title = "SVM")


# Check residuals
check_residuals(ypred = pred, ytrue = ts_test[,"consumption"], freq = 96)

```


## Neural Networks

```{r}

#nn_cv = function(x, h){forecast(nnetar(x, size = 35, p = 10, P = 2),h=h)}
#err1_nn = tsCV(ts_train[,"consumption"], nn_cv, h = 96)
#errors_df = rbind(errors_df, new_error_row(error = err1_nn, 
                                 #          model = "Neural Networks", 
                                  #         is_covar = FALSE))

# Fit
nn_fit = nnetar(ts_train[,"consumption"], size = 35, p = 10, P = 2)

# Forecast
pred = forecast(nn_fit, h = test_horizon)$mean

# Calculate and store error
mse = mse_error(pred, ts_test[,"consumption"])
cat("MSE Neural Networks: ", mse)
errors_df = rbind(errors_df, data.frame(
                                        method = "Neural Networks",
                                        covariate = FALSE,
                                        MSE = mse))

# Check residuals
check_residuals(pred, ts_test[,"consumption"],freq = 96)

# Plot Forecast
plot_forecast(train = ts_train[,"consumption"], test = ts_test[,"consumption"], pred = pred, xlim = c(40, 55), title = "Neurala Network Forecast")

```

# Error Comparison (without covariate)

```{r}
sorted_error = errors_df[order(errors_df$MSE), ]
sorted_error
```

############################################################################################################################################################################################
############################################################################################################################################################################################
############################################################################################################################################################################################+

## With Covariate Temperature

ts_future_temp


### SARIMA
#### Auto.arima
```{r}
fit_auto_xreg = auto.arima(ts_train[,"consumption"], xreg = ts_train[,"temperature"])
checkresiduals(fit_auto_xreg)

```

#### Improved Auto.arima
```{r}
# Fit
arima_fit_xreg = Arima(ts_train[,"consumption"], order = c(1,0,2), seasonal = c(0,1,1), xreg = ts_train[,"temperature"])

# Forecast
pred = forecast(arima_fit_xreg, h = test_horizon, xreg = ts_test[,"temperature"])$mean

# Calculate and store error
mse = mse_error(pred, ts_test[,"consumption"])
cat("MSE Arima(1,0,2)(0,1,1) XREG: ", mse)
errors_df = rbind(errors_df, data.frame(
                                        method = "Arima(1,0,2)(0,1,1)[96]",
                                        covariate = TRUE,
                                        MSE = mse))

# Check residuals
checkresiduals(arima_fit_xreg)

# Plot Forecast
plot_forecast(train = ts_train[,"consumption"], test = ts_test[,"consumption"], pred = pred, xlim = c(40, 55), title = "Arima(1,0,2)(0,1,1)[96]")

```

## TSLM

```{r}

```


## Machine Learning Models

### ML Data Preparation

#### New ML Compatible dataset with covariate

Data Transformation into ML data set, including the temperature as the 97th variable. 98th variable is the X_T+1

```{r}

vec_train = as.vector(ts_train[,"consumption"])
vec_xreg = as.vector(ts_train[,"temperature"])

ts_ml_xreg = c(vec_train[1:97], vec_xreg[96])

# Swapping columns, placing the response variable as the last column]
swap = vec_xreg[96]
ts_ml_xreg[98] =  ts_ml_xreg[97]
ts_ml_xreg[97] = swap

for(i in 1:(length(vec_train)-97))
{
    newdata = c(vec_train[(i+1):(i+97)],vec_xreg[i+96])
    swap = vec_xreg[i+96]
    newdata[98] = newdata[97]
    newdata[97] = swap
    
    ts_ml_xreg = rbind(ts_ml_xreg, newdata)
}

```


Support function to predict with covariates

```{r}
predMl_xreg = function(h, freq, pred_start,train, xreg_data, model, matrix = FALSE)
{
  pred = rep(NULL, h)
  # The first prediction will be the last row of  the training data set. 
  newdata = c(tail(train, freq), xreg_data[1])
  
  for (t in 1:(h)){
    
    # To deal with the cases where we need to have a matrix for prediction (like in SVM)
    if(matrix == TRUE)
    {
      newdata = matrix(newdata,1,freq+1) #+1 to account for the xreg
    }
    pred[t] = predict(model, newdata = newdata)
    
    # The following element forecast will be from the second element until the previously predicted value (and so on)
    if(t+1 <= length(xreg_data)){
      # Remove the first and last (temperature) element
      newdata = newdata[-c(1, length(newdata))]
      # Add new prediction and new future temperature value
      newdata = c(newdata, pred[t], xreg_data[t+1])
    }
  }
  
  # Finally, we transform the predictions into a Time-Series again
  pred_ts = ts(pred, start = pred_start, freq = freq)
  
  return(pred_ts)
  
}
```



### Random Forest

```{r}

# Fit
rf_fit_xreg = randomForest(x = ts_ml_xreg[,-98], y=ts_ml_xreg[,98], ntree = 1000)

# Forecast
pred = predMl_xreg(h = test_horizon, freq = 96, pred_start = c(41,1), train = ts_train[,"consumption"], xreg_data = ts_test[,"temperature"], model = rf_fit_xreg)

# Calculate and store error
mse = mse_error(pred, ts_test[,"consumption"])
cat("Random Forest with XREG MSE: ", mse)
errors_df = rbind(errors_df, data.frame(
                                        method = "Random Forest XREG",
                                        covariate = TRUE,
                                        MSE = mse))
# Check residuals
check_residuals(pred, ts_test[,"consumption"],freq = 96)

# Plot Forecast
plot_forecast(train = ts_train[,"consumption"], test = ts_test[,"consumption"], pred = pred, title = "Random Forest [TEMP]")

```

### Gradient Boosting

```{r}

# Fit
xg_fit_xreg = xgboost(data = ts_ml_xreg[,-98], label = ts_ml_xreg[,98],
                                            max_depth = 10, 
                                            eta = .5, nrounds = 50,
                                            nthread = 2, 
                                            objective = "reg:squarederror", 
                                            verbose = FALSE)
# Forecast
pred = predMl_xreg(h = test_horizon, freq = 96, pred_start = c(41,1), train = ts_train[,"consumption"], xreg_data = ts_test[,"temperature"], model = xg_fit_xreg, matrix = TRUE)

# Error
mse = mse_error(pred, ts_test[,"consumption"])
cat("XGBoost XREG MSE: ", mse)
errors_df = rbind(errors_df, data.frame(
                                        method = "XGBoost XREG",
                                        covariate = TRUE,
                                        MSE = mse))

# Check Residuals
check_residuals(ypred = pred, ytrue = ts_test[,"consumption"], freq = 96)

# Plot Forecast
plot_forecast(train = ts_train[,"consumption"], test = ts_test[,"consumption"], pred = pred, title = "XGBoost [TEMP]")
```


### SVM

```{r}
# Fit
svm_fit_xreg = svm(x = ts_ml_xreg[,-98], y = ts_ml_xreg[,98])
# Forecast
pred = predMl_xreg(h = test_horizon, freq = 96, pred_start = c(41,1), train = ts_train[,"consumption"], xreg_data = ts_test[,"temperature"], model = svm_fit_xreg, matrix = TRUE)

# Error
mse = mse_error(pred, ts_test[,"consumption"])
cat("SVM XREG MSE: ", mse)
errors_df = rbind(errors_df, data.frame(
                                        method = "SVM XREG",
                                        covariate = TRUE,
                                        MSE = mse))

# Check Residuals
check_residuals(ypred = pred, ytrue = ts_test[,"consumption"], freq = 96)

# Plot Forecast
plot_forecast(train = ts_train[,"consumption"], test = ts_test[,"consumption"], pred = pred, title = "SVM [TEMP]")
```


## Neural Netwroks

```{r}
# Fit
nn_fit_xreg = nnetar(ts_train[,"consumption"], size = 35, p = 10, P = 2, xreg = ts_train[,"temperature"])


# Forecast
pred = forecast(nn_fit_xreg, h = test_horizon, xreg = ts_test[,"temperature"])$mean

# Calculate and store error
mse = mse_error(pred, ts_test[,"consumption"])
cat("MSE Neural Networks: ", mse)
errors_df = rbind(errors_df, data.frame(
                                        method = "Neural Networks",
                                        covariate = TRUE,
                                        MSE = mse))

# Check residuals
check_residuals(pred, ts_test[,"consumption"],freq = 96)

# Plot Forecast
plot_forecast(train = ts_train[,"consumption"], test = ts_test[,"consumption"], pred = pred, xlim = c(40, 55), title = "Neural Network XREG Forecast")

```



# Model Selection
## Error Comparison
```{r}

sorted_error = errors_df[order(errors_df$MSE), ]
sorted_error

```

## Cross-Validation with the best model


```{r}


train_splits = list(c(35,96),c(37,96),c(39,96),c(42,96),c(45,96),c(47,96))
test_splits = list(c(36,1),c(38,1),c(40,1),c(43,1),c(46,1),c(48,1))
splits = 6

error = c()

for (i in (1 : splits)){
  
    # New Split
    ts_cv_train = window(ts, end = train_splits[[i]])
    ts_cv_test = window(ts, start = test_splits[[i]])
  
    # Train
    fit_cv = Arima(ts_cv_train[,"consumption"], order = c(1,0,2), seasonal = c(0,1,1), xreg = ts_cv_train[,"temperature"])
    
    # Forecast
    pred = forecast(fit_cv, h = length(ts_cv_test[,"consumption"]), xreg = ts_cv_test[,"temperature"])
   
    # Calculate error
    mse = mse_error(pred$mean, ts_cv_test[,"consumption"])  
    error = rbind(error, mse)
    
    cat(mse, "\n")
}

cat("CV Average:", mean(error))
```


# Best Model

Training the best model in the whole dataset

```{r}
# Avoid warning for different column names
xreg = as.matrix(ts[,"temperature"])
colnames(xreg) = c("temperature")

best_model_xreg = Arima(ts[,"consumption"], order = c(1,0,2), seasonal = c(0,1,1), xreg = xreg)
best_model = Arima(ts[,"consumption"], order = c(1,0,2), seasonal = c(0,1,1))
```

## Forecasting Final Results

```{r}
xreg = future_temperature_ts
colnames(xreg) = c("temperature")

final_pred = forecast(best_model, h = 96)$mean
final_pred_xreg = forecast(best_model_xreg, xreg = future_temperature_ts)$mean

forescasts = data.frame(
    withoutTemp = final_pred,
    withTemp = final_pred_xreg
  )

```


# Exporting Results

Saving Excell File
```{r}
library(openxlsx)

write.xlsx(forescasts, "CarlosTussiLeite.xlsx", colNames = FALSE)
```

